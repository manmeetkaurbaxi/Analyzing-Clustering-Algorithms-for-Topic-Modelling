# -*- coding: utf-8 -*-
"""AG News-Doc2Vec.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/manmeetkaurbaxi/Analyzing-Clustering-Algorithms-for-Topic-Modelling/blob/master/code/Finding%20optimal%20epochs/AG%20News-BagOfWords.ipynb
"""

# !pip install gensim --upgrade -q

import gensim
import pandas as pd
from sklearn import metrics
from operator import itemgetter
import tqdm
import nltk
import re
from gensim.models.nmf import Nmf

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

def preprocess_text(sentence):
    # Lowercase
    sentence = sentence.lower()
    
    # Remove all non-alphabets (punctuation, numbers, new-line characters and extra-spaces)
    sentence = re.sub(r'[^a-zA-Z]+', ' ', sentence)
    sentence = sentence.replace('\n', '')
    # sentence = re.sub('\s\s+', ' ', sentence)
    
    # Tokenize & remove stop-words
    word_list = nltk.word_tokenize(sentence)    
    stopwords_list = set(nltk.corpus.stopwords.words('english'))
    word_list = [word for word in word_list if word not in stopwords_list]
    
    # Remove very small words, length < 3, they don't contribute any useful information
    word_list = [word for word in word_list if len(word) > 3]
        
    # Stem & Lemmatize
    porter_stemmer = nltk.stem.PorterStemmer()
    lemmatizer = nltk.stem.WordNetLemmatizer()
    word_list = [porter_stemmer.stem(word) for word in word_list]
    word_list = [lemmatizer.lemmatize(word) for word in word_list]
    
    sentence = ' '.join(word_list)
    
    return sentence

"""### Load Data"""

train_df = pd.read_csv('train.csv')

for index, row in train_df.iterrows():
    if row['class'] == 1:
        train_df.at[index, 'topic'] = 'World'
    elif row['class'] == 2:
        train_df.at[index, 'topic'] = 'Sports'
    elif row['class'] == 3:
        train_df.at[index, 'topic'] = 'Business'
    else:
        train_df.at[index, 'topic'] = 'Sci/Tech'

# Preprocess the news description
tqdm.tqdm.pandas()
train_df['news_tokenized'] = train_df['description'].progress_apply(lambda x: preprocess_text(str(x)))

"""### Bag of Words"""

train_documents = train_df['news_tokenized'].str.split()
dictionary = gensim.corpora.Dictionary(train_documents)
dictionary.filter_extremes(no_below=5, no_above=0.5, keep_n=20000)

train_corpus = [dictionary.doc2bow(document) for document in train_documents]

"""### Find optimal epochs for NMF"""

performance_metrics = pd.DataFrame(columns=['feature-extraction','run#', 'epoch', 'state', 'AMI','ARI','NMI'])

for run in range(1, 21, 1):
    print('Run #', run)
    for epoch, state in zip(range(25, 300, 15), range(2, 42, 2)):
        pred_labels = []
        print('Epoch:', epoch)
        print('State:', state)
        gensim_nmf = Nmf(corpus=train_corpus, num_topics=4, id2word=dictionary, chunksize=1000, passes=epoch, eval_every=10, minimum_probability=0, 
                                   random_state=state, kappa=1)
        
        for train_doc in train_corpus:
            pred_label = max(gensim_nmf[train_doc], key=itemgetter(1))[0]
            pred_labels.append(pred_label)
        
        ami = metrics.adjusted_mutual_info_score(train_df['class'], pred_labels)
        ari = metrics.adjusted_rand_score(train_df['class'], pred_labels)
        nmi = metrics.normalized_mutual_info_score(train_df['class'], pred_labels)

        # print(run, epoch, state)
        
        performance_metrics = performance_metrics.append({'feature-extraction':'Bag of Words', 'run#':run, 'epoch':epoch, 'state':state, 'AMI':'{:f}'.format(ami), 'ARI':'{:f}'.format(ari), 'NMI':'{:f}'.format(nmi)}, ignore_index=True)

performance_metrics.to_csv('performance-bow_nmf.csv', index=False)

performance_metrics[['run#','epoch','state','AMI','ARI','NMI']] = performance_metrics[['run#','epoch','state','AMI','ARI','NMI']].apply(pd.to_numeric, errors='coerce')

performance_metrics

mean_performance = performance_metrics.groupby('epoch', as_index=False)[['AMI','ARI','NMI']].mean()

mean_performance.to_csv('mean_performance-bow_nmf.csv', index=False)

